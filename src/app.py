"""
Text Analyzer - Streamlit App
Supports .txt, .pdf, and .docx files
Performs tokenization and lemmatization for Russian and Belarusian languages
Uses pymorphy3 (Russian) and lemmatizer_be (Belarusian)
"""

# Core Streamlit and data processing imports
import streamlit as st
from collections import Counter
import re
import io
import csv

# File reading imports
import PyPDF2
from docx import Document
from io import BytesIO

# Language support modules
from ru_support import lemmatize_russian, get_russian_stop_words
from be_support import lemmatize_belarusian, get_belarusian_stop_words


def read_txt_file(file):
    """
    Read content from a .txt file with automatic encoding detection
    
    Args:
        file: File object from Streamlit file uploader
        
    Returns:
        str: Decoded text content
    """
    try:
        # Try UTF-8 encoding first (most common)
        content = file.read().decode('utf-8')
    except UnicodeDecodeError:
        # Fallback to Windows-1251 (common for Cyrillic text)
        file.seek(0)  # Reset file pointer to beginning
        content = file.read().decode('cp1251', errors='ignore')
    return content


def read_pdf_file(file):
    """
    Read content from a .pdf file
    
    Args:
        file: File object from Streamlit file uploader
        
    Returns:
        str: Extracted text from all PDF pages
    """
    # Create PDF reader from bytes
    pdf_reader = PyPDF2.PdfReader(BytesIO(file.read()))
    content = ""
    # Extract text from each page
    for page in pdf_reader.pages:
        content += page.extract_text() + "\n"
    return content


def read_docx_file(file):
    """
    Read content from a .docx file
    
    Args:
        file: File object from Streamlit file uploader
        
    Returns:
        str: Extracted text from all DOCX paragraphs
    """
    # Create Document object from bytes
    doc = Document(BytesIO(file.read()))
    content = ""
    # Extract text from each paragraph
    for paragraph in doc.paragraphs:
        content += paragraph.text + "\n"
    return content


def read_file_content(uploaded_file):
    """
    Read file content based on file type
    
    Args:
        uploaded_file: Streamlit UploadedFile object
        
    Returns:
        str: Extracted text content from the file
        
    Raises:
        ValueError: If file type is not supported
    """
    # Extract file extension from filename
    file_extension = uploaded_file.name.split('.')[-1].lower()
    
    # Route to appropriate reader based on file type
    if file_extension == 'txt':
        return read_txt_file(uploaded_file)
    elif file_extension == 'pdf':
        return read_pdf_file(uploaded_file)
    elif file_extension == 'docx':
        return read_docx_file(uploaded_file)
    else:
        raise ValueError(f"Unsupported file type: {file_extension}")


def tokenize_text(text):
    """
    Tokenize text into words
    Removes punctuation and keeps only Cyrillic and Latin letters
    
    Args:
        text: Raw text string to tokenize
        
    Returns:
        list: List of lowercase words (Cyrillic and Latin only)
    """
    # Extract words using regex: Cyrillic (–∞-—è, —ë) and Latin (a-z) letters only
    words = re.findall(r'[–∞-—è—ë–ê-–Ø–Åa-zA-Z]+', text)
    # Convert all words to lowercase for consistency
    words = [word.lower() for word in words]
    return words


def filter_stop_words(lemmas, stop_words):
    """
    Filter out stop words from the list of lemmas
    
    Args:
        lemmas: List of lemmatized words
        stop_words: Set of stop words to filter out
        
    Returns:
        list: Filtered list with stop words removed
    """
    return [lemma for lemma in lemmas if lemma not in stop_words]


def create_csv_download(freq_data, filename):
    """
    Create CSV data for download
    
    Args:
        freq_data: Dictionary with –†–∞–Ω–≥, –õ–µ–º–º–∞, –ß–∞—Å—Ç–æ—Ç–∞
        filename: Name for the downloaded file
        
    Returns:
        CSV data as bytes
    """
    output = io.StringIO()
    writer = csv.writer(output)
    
    # Write header
    writer.writerow(['–†–∞–Ω–≥', '–õ–µ–º–º–∞', '–ß–∞—Å—Ç–æ—Ç–∞'])
    
    # Write data rows
    for i in range(len(freq_data['–†–∞–Ω–≥'])):
        writer.writerow([
            freq_data['–†–∞–Ω–≥'][i],
            freq_data['–õ–µ–º–º–∞'][i],
            freq_data['–ß–∞—Å—Ç–æ—Ç–∞'][i]
        ])
    
    return output.getvalue().encode('utf-8-sig')  # BOM for Excel compatibility


def main():
    """Main Streamlit application"""
    
    # Page configuration
    st.set_page_config(
        page_title="Text Analyzer",
        page_icon="üìù",
        layout="wide"
    )
    
    # Header section
    st.title("üìù Text Analyzer")
    st.markdown("""
    –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª (.txt, .pdf –∏–ª–∏ .docx) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
    –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–∏—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é –∏ —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
    """)
    
    # Language selector - Belarusian support currently disabled
    # The commented code below allows users to choose between Russian and Belarusian
    # Uncomment to enable multi-language selection:
    # st.markdown("### üåç –í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞")
    # language = st.radio(
    #     "–í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ –≤–∞—à–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:",
    #     options=["–†—É—Å—Å–∫–∏–π (Russian)", "–ë–µ–ª–∞—Ä—É—Å–∫–∞—è (Belarusian)"],
    #     horizontal=True,
    #     help="‚ö†Ô∏è –í–ê–ñ–ù–û: –í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —è–∑—ã–∫ –ü–ï–†–ï–î –∞–Ω–∞–ª–∏–∑–æ–º —Ç–µ–∫—Å—Ç–∞!"
    # )
    # lang_code = "ru" if "–†—É—Å—Å–∫–∏–π" in language else "be"
    
    # Currently hardcoded to Russian only
    lang_code = "ru"
    
    # Display selected language to user
    lang_emoji = "üá∑üá∫"
    lang_name = "–†—É—Å—Å–∫–∏–π"
    st.info(f"{lang_emoji} **–Ø–∑—ã–∫ –∞–Ω–∞–ª–∏–∑–∞:** {lang_name}")
    
    # File uploader
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª",
        type=['txt', 'pdf', 'docx'],
        help="–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª .txt, .pdf –∏–ª–∏ .docx —Å —Ä—É—Å—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º"
    )
    
    # Process file if user has uploaded one
    if uploaded_file is not None:
        # Show success message with filename
        st.success(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: **{uploaded_file.name}**")
        
        # Display spinner during processing
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞..."):
            try:
                # Step 1: Extract text from file (supports .txt, .pdf, .docx)
                text_content = read_file_content(uploaded_file)
                
                # Step 2: Tokenize text into individual words
                words = tokenize_text(text_content)
                
                # Step 3: Lemmatize words based on selected language
                if lang_code == "ru":
                    # Russian: use pymorphy3 for morphological analysis
                    lemmas = lemmatize_russian(words)
                    stop_words = get_russian_stop_words()
                else:
                    # Belarusian: use lemmatizer_be based on Bnkorpus
                    lemmas = lemmatize_belarusian(words)
                    stop_words = get_belarusian_stop_words()
                
                # Step 4: Remove stop words (prepositions, conjunctions, etc.)
                filtered_lemmas = filter_stop_words(lemmas, stop_words)
                
                # Step 5: Calculate statistics for analysis
                total_words = len(words)  # Total word count
                unique_lemmas = len(set(filtered_lemmas))  # Count of unique lemmas
                lemma_freq = Counter(filtered_lemmas)  # Frequency distribution
                top_50_lemmas = lemma_freq.most_common(50)  # Top 50 most frequent
                
                # Display results section
                st.markdown("---")
                st.header("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
                
                # Show how many stop words were filtered out
                filtered_count = len(lemmas) - len(filtered_lemmas)
                st.info(f"üîç –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count} —Å—Ç–æ–ø-—Å–ª–æ–≤ ({(filtered_count/len(lemmas)*100):.1f}% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞)")
                
                # Display three key metrics in columns
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(
                        label="–í—Å–µ–≥–æ —Å–ª–æ–≤",
                        value=f"{total_words:,}",
                        help="–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"
                    )
                with col2:
                    st.metric(
                        label="–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º",
                        value=f"{unique_lemmas:,}",
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º"
                    )
                with col3:
                    # Lexical diversity = ratio of unique lemmas to total words
                    st.metric(
                        label="–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ",
                        value=f"{(unique_lemmas / total_words * 100):.1f}%",
                        help="–û—Ç–Ω–æ—à–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º –∫ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤"
                    )
                
                st.markdown("---")
                
                # Display top 50 most frequent lemmas
                st.subheader("üîù –¢–æ–ø-50 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö –ª–µ–º–º")
                
                # Prepare data for table display (rank, lemma, frequency)
                freq_data = {
                    "–†–∞–Ω–≥": list(range(1, len(top_50_lemmas) + 1)),
                    "–õ–µ–º–º–∞": [lemma for lemma, _ in top_50_lemmas],
                    "–ß–∞—Å—Ç–æ—Ç–∞": [freq for _, freq in top_50_lemmas]
                }
                st.table(freq_data)
                
                # Create CSV download button for exporting results
                csv_data = create_csv_download(freq_data, "results.csv")
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
                    data=csv_data,
                    file_name=f"text_analysis_{uploaded_file.name.rsplit('.', 1)[0]}.csv",
                    mime="text/csv",
                    help="–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É —á–∞—Å—Ç–æ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV –¥–ª—è Excel"
                )
                
                # Optional: Show preview of original text in expandable section
                with st.expander("üìÑ –ü—Ä–æ—Å–º–æ—Ç—Ä –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤)"):
                    preview_text = text_content[:500]
                    if len(text_content) > 500:
                        preview_text += "..."
                    st.text(preview_text)
                
            except Exception as e:
                # Display error message if something goes wrong during processing
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}")
                st.exception(e)


if __name__ == "__main__":
    main()

