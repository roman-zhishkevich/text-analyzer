"""
Text Analyzer - Streamlit App
Supports .txt, .pdf, and .docx files
Performs tokenization and lemmatization for Russian and Belarusian languages
Uses pymorphy3 (Russian) and lemmatizer_be (Belarusian)
"""

import streamlit as st
from collections import Counter
import re
import io
import csv

# File reading imports
import PyPDF2
from docx import Document
from io import BytesIO

# Language support modules
from ru_support import lemmatize_russian, get_russian_stop_words
from be_support import lemmatize_belarusian, get_belarusian_stop_words


def read_txt_file(file):
    """Read content from a .txt file"""
    try:
        # Try UTF-8 first, then fall back to other encodings
        content = file.read().decode('utf-8')
    except UnicodeDecodeError:
        file.seek(0)
        content = file.read().decode('cp1251', errors='ignore')
    return content


def read_pdf_file(file):
    """Read content from a .pdf file"""
    pdf_reader = PyPDF2.PdfReader(BytesIO(file.read()))
    content = ""
    for page in pdf_reader.pages:
        content += page.extract_text() + "\n"
    return content


def read_docx_file(file):
    """Read content from a .docx file"""
    doc = Document(BytesIO(file.read()))
    content = ""
    for paragraph in doc.paragraphs:
        content += paragraph.text + "\n"
    return content


def read_file_content(uploaded_file):
    """Read file content based on file type"""
    file_extension = uploaded_file.name.split('.')[-1].lower()
    
    if file_extension == 'txt':
        return read_txt_file(uploaded_file)
    elif file_extension == 'pdf':
        return read_pdf_file(uploaded_file)
    elif file_extension == 'docx':
        return read_docx_file(uploaded_file)
    else:
        raise ValueError(f"Unsupported file type: {file_extension}")


def tokenize_text(text):
    """
    Tokenize text into words
    Removes punctuation and keeps only Cyrillic and Latin letters
    """
    # Extract words (Cyrillic and Latin letters)
    words = re.findall(r'[–∞-—è—ë–ê-–Ø–Åa-zA-Z]+', text)
    # Convert to lowercase
    words = [word.lower() for word in words]
    return words


def filter_stop_words(lemmas, stop_words):
    """
    Filter out stop words from the list of lemmas
    """
    return [lemma for lemma in lemmas if lemma not in stop_words]


def create_csv_download(freq_data, filename):
    """
    Create CSV data for download
    
    Args:
        freq_data: Dictionary with –†–∞–Ω–≥, –õ–µ–º–º–∞, –ß–∞—Å—Ç–æ—Ç–∞
        filename: Name for the downloaded file
        
    Returns:
        CSV data as bytes
    """
    output = io.StringIO()
    writer = csv.writer(output)
    
    # Write header
    writer.writerow(['–†–∞–Ω–≥', '–õ–µ–º–º–∞', '–ß–∞—Å—Ç–æ—Ç–∞'])
    
    # Write data rows
    for i in range(len(freq_data['–†–∞–Ω–≥'])):
        writer.writerow([
            freq_data['–†–∞–Ω–≥'][i],
            freq_data['–õ–µ–º–º–∞'][i],
            freq_data['–ß–∞—Å—Ç–æ—Ç–∞'][i]
        ])
    
    return output.getvalue().encode('utf-8-sig')  # BOM for Excel compatibility


def main():
    """Main Streamlit application"""
    
    # Page configuration
    st.set_page_config(
        page_title="Text Analyzer",
        page_icon="üìù",
        layout="wide"
    )
    
    # Title and description
    st.title("üìù Text Analyzer")
    st.markdown("""
    –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª (.txt, .pdf –∏–ª–∏ .docx) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
    –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–∏—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é –∏ —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
    """)
    
    # Language selector - Belarusian support disabled
    # st.markdown("### üåç –í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞")
    # language = st.radio(
    #     "–í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ –≤–∞—à–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:",
    #     options=["–†—É—Å—Å–∫–∏–π (Russian)", "–ë–µ–ª–∞—Ä—É—Å–∫–∞—è (Belarusian)"],
    #     horizontal=True,
    #     help="‚ö†Ô∏è –í–ê–ñ–ù–û: –í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —è–∑—ã–∫ –ü–ï–†–ï–î –∞–Ω–∞–ª–∏–∑–æ–º —Ç–µ–∫—Å—Ç–∞!"
    # )
    # lang_code = "ru" if "–†—É—Å—Å–∫–∏–π" in language else "be"
    
    # Hardcoded to Russian only
    lang_code = "ru"
    
    # Show selected language
    lang_emoji = "üá∑üá∫"
    lang_name = "–†—É—Å—Å–∫–∏–π"
    st.info(f"{lang_emoji} **–Ø–∑—ã–∫ –∞–Ω–∞–ª–∏–∑–∞:** {lang_name}")
    
    # File uploader
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª",
        type=['txt', 'pdf', 'docx'],
        help="–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª .txt, .pdf –∏–ª–∏ .docx —Å —Ä—É—Å—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º"
    )
    
    if uploaded_file is not None:
        # Display file information
        st.success(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: **{uploaded_file.name}**")
        
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞..."):
            try:
                # Read file content
                text_content = read_file_content(uploaded_file)
                
                # Tokenize text
                words = tokenize_text(text_content)
                
                # Lemmatize words based on selected language
                if lang_code == "ru":
                    # Russian: use pymorphy3
                    lemmas = lemmatize_russian(words)
                    stop_words = get_russian_stop_words()
                else:
                    # Belarusian: use lemmatizer_be
                    lemmas = lemmatize_belarusian(words)
                    stop_words = get_belarusian_stop_words()
                
                # Filter stop words (always enabled)
                filtered_lemmas = filter_stop_words(lemmas, stop_words)
                
                # Calculate statistics
                total_words = len(words)
                unique_lemmas = len(set(filtered_lemmas))
                lemma_freq = Counter(filtered_lemmas)
                top_20_lemmas = lemma_freq.most_common(20)
                
                # Display results
                st.markdown("---")
                st.header("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
                
                # Show filter info
                filtered_count = len(lemmas) - len(filtered_lemmas)
                st.info(f"üîç –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count} —Å—Ç–æ–ø-—Å–ª–æ–≤ ({(filtered_count/len(lemmas)*100):.1f}% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞)")
                
                # Metrics section
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(
                        label="–í—Å–µ–≥–æ —Å–ª–æ–≤",
                        value=f"{total_words:,}",
                        help="–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"
                    )
                with col2:
                    st.metric(
                        label="–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º",
                        value=f"{unique_lemmas:,}",
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º"
                    )
                with col3:
                    st.metric(
                        label="–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ",
                        value=f"{(unique_lemmas / total_words * 100):.1f}%",
                        help="–û—Ç–Ω–æ—à–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º –∫ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤"
                    )
                
                st.markdown("---")
                
                # Top 20 most frequent lemmas
                st.subheader("üîù –¢–æ–ø-20 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö –ª–µ–º–º")
                
                # Create a formatted table
                freq_data = {
                    "–†–∞–Ω–≥": list(range(1, len(top_20_lemmas) + 1)),
                    "–õ–µ–º–º–∞": [lemma for lemma, _ in top_20_lemmas],
                    "–ß–∞—Å—Ç–æ—Ç–∞": [freq for _, freq in top_20_lemmas]
                }
                st.table(freq_data)
                
                # Download button for CSV
                csv_data = create_csv_download(freq_data, "results.csv")
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
                    data=csv_data,
                    file_name=f"text_analysis_{uploaded_file.name.rsplit('.', 1)[0]}.csv",
                    mime="text/csv",
                    help="–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É —á–∞—Å—Ç–æ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV –¥–ª—è Excel"
                )
                
                # Optional: Display raw text preview
                with st.expander("üìÑ –ü—Ä–æ—Å–º–æ—Ç—Ä –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤)"):
                    preview_text = text_content[:500]
                    if len(text_content) > 500:
                        preview_text += "..."
                    st.text(preview_text)
                
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}")
                st.exception(e)
    else:
        # Show example/demo information
        with st.expander("‚ÑπÔ∏è –û –ø—Ä–æ–≥—Ä–∞–º–º–µ"):
            st.markdown("""
            ### –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
            - **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è pymorphy3 –¥–ª—è —Ç–æ—á–Ω–æ–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
            - **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤**: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ .txt, .pdf –∏ .docx
            - **–¢–æ—á–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è**: –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å —É—á–µ—Ç–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–∞
            - **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤**: –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–≥–æ–≤, —Å–æ—é–∑–æ–≤ –∏ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤
            - **–û—Ñ—Ñ–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∞**: –í—Å—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ –≤–∞—à–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ
            - **–ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑**: –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ –∏—Ö –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ
            
            ### –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:
            1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –ø–æ–º–æ—â—å—é —Ñ–æ—Ä–º—ã –≤—ã—à–µ
            2. –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç
            3. –¢–µ–∫—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è (—Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–æ–≤–∞)
            4. –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pymorphy3
            5. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            
            ### –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:
            - **–†—É—Å—Å–∫–∏–π —è–∑—ã–∫**: pymorphy3 —Å —Ä—É—Å—Å–∫–∏–º–∏ —Å–ª–æ–≤–∞—Ä—è–º–∏
            - **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ PDF**: PyPDF2
            - **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ DOCX**: python-docx
            """)


if __name__ == "__main__":
    main()

