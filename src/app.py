"""
Text Analyzer - Streamlit App
Supports .txt, .pdf, and .docx files
Performs tokenization and lemmatization for Russian and Belarusian languages
Uses pymorphy3 (Russian) and lemmatizer_be (Belarusian)
"""

# Core Streamlit and data processing imports
import streamlit as st
from collections import Counter
import re
import io
import csv

# Language support modules
from ru_support import lemmatize_russian, get_russian_stop_words
from belarusian.be_support import lemmatize_belarusian, get_belarusian_stop_words
from stop_words_manager import render_stop_words_ui
from text_input_handler import render_text_input_ui


def tokenize_text(text):
    """
    Tokenize text into words
    Removes punctuation and keeps only Cyrillic and Latin letters
    
    Args:
        text: Raw text string to tokenize
        
    Returns:
        list: List of lowercase words (Cyrillic and Latin only)
    """
    # Extract words using regex: Cyrillic (–∞-—è, —ë) and Latin (a-z) letters only
    words = re.findall(r'[–∞-—è—ë–ê-–Ø–Åa-zA-Z]+', text)
    # Convert all words to lowercase for consistency
    words = [word.lower() for word in words]
    return words


def filter_stop_words(lemmas, stop_words):
    """
    Filter out stop words from the list of lemmas
    
    Args:
        lemmas: List of lemmatized words
        stop_words: Set of stop words to filter out
        
    Returns:
        list: Filtered list with stop words removed
    """
    return [lemma for lemma in lemmas if lemma not in stop_words]


def create_csv_download(freq_data, filename):
    """
    Create CSV data for download
    
    Args:
        freq_data: Dictionary with –†–∞–Ω–≥, –õ–µ–º–º–∞, –ß–∞—Å—Ç–æ—Ç–∞
        filename: Name for the downloaded file
        
    Returns:
        CSV data as bytes
    """
    output = io.StringIO()
    writer = csv.writer(output)
    
    # Write header
    writer.writerow(['–†–∞–Ω–≥', '–õ–µ–º–º–∞', '–ß–∞—Å—Ç–æ—Ç–∞'])
    
    # Write data rows
    for i in range(len(freq_data['–†–∞–Ω–≥'])):
        writer.writerow([
            freq_data['–†–∞–Ω–≥'][i],
            freq_data['–õ–µ–º–º–∞'][i],
            freq_data['–ß–∞—Å—Ç–æ—Ç–∞'][i]
        ])
    
    return output.getvalue().encode('utf-8-sig')  # BOM for Excel compatibility


def main():
    """Main Streamlit application"""
    
    # Page configuration
    st.set_page_config(
        page_title="Text Analyzer",
        page_icon="üìù",
        layout="wide"
    )
    
    # Header section
    st.title("üìù Text Analyzer")
    st.markdown("""
    –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª (.txt, .pdf –∏–ª–∏ .docx) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
    –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–∏—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é –∏ —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
    """)
    
    # Language selector
    st.markdown("### üåç –í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞")
    language = st.radio(
        "–í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ –≤–∞—à–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:",
        options=["üá∑üá∫ –†—É—Å—Å–∫–∏–π", "üáßüáæ –ë–µ–ª–∞—Ä—É—Å–∫–∞—è"],
        horizontal=True,
        help="‚ö†Ô∏è –í–ê–ñ–ù–û: –í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —è–∑—ã–∫ –ü–ï–†–ï–î –∞–Ω–∞–ª–∏–∑–æ–º —Ç–µ–∫—Å—Ç–∞!"
    )
    
    # Determine language code and display information
    if "–†—É—Å—Å–∫–∏–π" in language:
        lang_code = "ru"
        lang_emoji = "üá∑üá∫"
        lang_name = "–†—É—Å—Å–∫–∏–π"
    else:
        lang_code = "be"
        lang_emoji = "üáßüáæ"
        lang_name = "–ë–µ–ª–∞—Ä—É—Å–∫–∞—è"
    
    st.info(f"{lang_emoji} **–Ø–∑—ã–∫ –∞–Ω–∞–ª–∏–∑–∞:** {lang_name}")
    
    # Render stop words management UI
    # This returns the combined set of default + custom stop words
    current_stop_words = render_stop_words_ui(lang_code)
    
    # Text input UI (file upload or direct paste)
    text_content, source_name = render_text_input_ui()
    
    # Process text if available
    if text_content is not None and text_content.strip():
        # Display spinner during processing
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞..."):
            try:
                # Step 1: Text is already extracted from render_text_input_ui()
                
                # Step 2: Tokenize text into individual words
                words = tokenize_text(text_content)
                
                # Step 3: Lemmatize words based on selected language
                if lang_code == "ru":
                    # Russian: use pymorphy3 for morphological analysis
                    lemmas = lemmatize_russian(words)
                else:
                    # Belarusian: use lemmatizer_be based on Bnkorpus
                    lemmas = lemmatize_belarusian(words)
                
                # Step 4: Remove stop words (prepositions, conjunctions, etc.)
                # Use stop words from the UI (includes custom additions)
                filtered_lemmas = filter_stop_words(lemmas, current_stop_words)
                
                # Step 5: Calculate statistics for analysis
                total_words = len(words)  # Total word count
                unique_lemmas = len(set(filtered_lemmas))  # Count of unique lemmas
                lemma_freq = Counter(filtered_lemmas)  # Frequency distribution
                top_50_lemmas = lemma_freq.most_common(50)  # Top 50 most frequent
                
                # Display results section
                st.markdown("---")
                st.header("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
                
                # Show how many stop words were filtered out
                filtered_count = len(lemmas) - len(filtered_lemmas)
                st.info(f"üîç –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count} —Å—Ç–æ–ø-—Å–ª–æ–≤ ({(filtered_count/len(lemmas)*100):.1f}% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞)")
                
                # Display three key metrics in columns
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(
                        label="–í—Å–µ–≥–æ —Å–ª–æ–≤",
                        value=f"{total_words:,}",
                        help="–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"
                    )
                with col2:
                    st.metric(
                        label="–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º",
                        value=f"{unique_lemmas:,}",
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º"
                    )
                with col3:
                    # Lexical diversity = ratio of unique lemmas to total words
                    st.metric(
                        label="–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ",
                        value=f"{(unique_lemmas / total_words * 100):.1f}%",
                        help="–û—Ç–Ω–æ—à–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º –∫ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤"
                    )
                
                st.markdown("---")
                
                # Display top 50 most frequent lemmas
                st.subheader("üîù –¢–æ–ø-50 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö –ª–µ–º–º")
                
                # Prepare data for table display (rank, lemma, frequency)
                freq_data = {
                    "–†–∞–Ω–≥": list(range(1, len(top_50_lemmas) + 1)),
                    "–õ–µ–º–º–∞": [lemma for lemma, _ in top_50_lemmas],
                    "–ß–∞—Å—Ç–æ—Ç–∞": [freq for _, freq in top_50_lemmas]
                }
                st.table(freq_data)
                
                # Create CSV download button for exporting results
                csv_data = create_csv_download(freq_data, "results.csv")
                # Generate filename from source (remove extension if present)
                safe_filename = source_name.rsplit('.', 1)[0] if '.' in source_name else source_name
                safe_filename = safe_filename.replace(' ', '_')
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
                    data=csv_data,
                    file_name=f"text_analysis_{safe_filename}.csv",
                    mime="text/csv",
                    help="–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É —á–∞—Å—Ç–æ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV –¥–ª—è Excel"
                )
                
                # Optional: Show preview of original text in expandable section
                with st.expander("üìÑ –ü—Ä–æ—Å–º–æ—Ç—Ä –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤)"):
                    preview_text = text_content[:500]
                    if len(text_content) > 500:
                        preview_text += "..."
                    st.text(preview_text)
                
            except Exception as e:
                # Display error message if something goes wrong during processing
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
                st.exception(e)


if __name__ == "__main__":
    main()

